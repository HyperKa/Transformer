# Java Transformer
![Java](https://img.shields.io/badge/Java-22-blue.svg)
![Maven](https://img.shields.io/badge/Maven-3.8+-orange.svg)
![GitHub Actions](https://img.shields.io/badge/build-passing-brightgreen.svg)

**Java Transformer** — это учебный проект, демонстрирующий полную реализацию архитектуры Трансформера с нуля на чистой Java, без использования высокоуровневых deep learning фреймворков, таких как TensorFlow или PyTorch. Проект включает в себя все ключевые компоненты: от предобработки данных до механизма многоголового внимания, обучения с помощью обратного распространения ошибки и использования обученной модели для предсказаний.

Основная задача модели — **классификация фрагментов Java-кода** на предмет наличия потенциальных уязвимостей (бинарная классификация).

## Часть 1: Архитектура и Принцип Работы

### 1.1. Стек Технологий

*   **Язык:** Java 22
*   **Сборка:** Apache Maven
*   **Линейная алгебра:** [Apache Commons Math](https://commons.apache.org/proper/commons-math/) — для всех матричных операций.
*   **Обработка данных:** Стандартные средства Java (RegEx, IO/NIO).
*   **Тестирование:** не добавлено.

### 1.2. Общая Архитектура

Модель представляет собой реализацию архитектуры **Encoder-Only Transformer**, схожей с BERT. Она состоит из следующих ключевых компонентов, которые вызываются последовательно:

`Данные -> Vocabulary -> EmbeddingTable -> PositionalEncoding -> N x EncoderBlock -> ClassificationHead -> Предсказание`

### 1.3. Порядок вызова классов и их описание

Ниже представлен подробный разбор того, как данные проходят через систему.

#### Сценарий A: Обучение (файлы `.ser` отсутствуют)

**`Main.java` (Точка входа для обучения):**
1.  **`TrainingData`**: Загружает сырой датасет из `.txt` файла. С помощью регулярных выражений парсит его на фрагменты кода и соответствующие им метки:
   `0` - безопасный код,
   `1` - утечка ресурсов,
   `2` - некорректная обработка исключений,
   `3` - XXE-инъекция,
   `4` - утечка ресурсов,
   `5` - риск нулевого значения Null Pointer Exception,
   `6` - SQL-инъекция,
   `7` - использование устаревшего API (импорта).
2.  **`Vocabulary`**: Принимает все токенизированные фрагменты кода. Создает уникальный словарь "слово -> индекс" (`wordToIndex`) и "индекс -> слово", резервируя специальные значения токенов сразу: `[PAD]` = 0, `[UNK]` = 1, `[SOS]` = 2, `[EOS]` = 3. Остальные токены (слова) добавляются в модель, если они не определены в словаре. В итоге каждый пример в датасете разделяется на два массива данных: числовая последовательность токенов (`tokenMatrix`) и маска внимания (`attentionMask`).
3.  **`TransformerModel` (Главный класс модели):**
    *   **`EmbeddingTable`**: Инициализирует матрицу эмбеддингов (`[vocab_size, embedding_dim]`) случайными значениями. На `forward` проходе по числовым индексам находит соответствующие им векторы.
    *   **`PositionalEncoding`**: Генерирует и добавляет к матрице эмбеддингов информацию о позиции каждого токена.
    *   **`EncoderBlock` (N раз)**: Сердце модели. Каждый блок состоит из:
        *   **`MultiHeadAttention`**: Принимает матрицу от предыдущего слоя (в первой итерации это . Внутри содержит `N` "голов". При выполнении всех голов происходит их конкатенация, на выходе получается матрица размером (`[vocab_size, embedding_dim]`).
            *   **`SelfAttention` (N раз)**: Каждая голова независимо вычисляет матрицы `Q`, `K`, `V` и веса внимания, создавая контекстно-зависимое представление.
        *   **`LayerNormalization`**: Стабилизирует выходы после `MultiHeadAttention` и `FeedForwardNetwork`.
        *   **`FeedForwardNetwork`**: Применяет двухслойную полносвязную сеть к каждому токену независимо.
    *   **`ClassificationHead`**: Простой полносвязный слой, который принимает выход для токена `[CLS]` от последнего `EncoderBlock` и преобразует его в "логиты" (сырые предсказания для каждого класса).
4.  **`CrossEntropyLoss`**: Принимает логиты, применяет к ним `Softmax` для получения вероятностей и сравнивает с истинной меткой для вычисления ошибки (`loss`). На `backward` проходе вычисляет стартовый градиент.
5.  **`OptimizerAdam` / `OptimizerSGD (в будущем)`**: Запускает `backward` проход по всей модели, который вычисляет градиенты для всех обучаемых параметров. Затем выполняет шаг градиентного спуска, обновляя веса.
6.  **Сохранение:** После завершения обучения `TransformerModel` и `Vocabulary` сериализуются и сохраняются в файлы (`.ser`).

#### Сценарий B: Предсказание (файлы `.ser` существуют)

**`Predict.java` (Точка входа для анализа):**
1.  **`CodeAnalyzer` (Инициализация):**
    *   **`TransformerModel.load()`**: Десериализует и загружает **полностью обученную** модель со всеми ее весами из файла с байт-кодом `transformer_model.ser`.
    *   **`Vocabulary.load()`**: Десериализует и загружает словарь, созданный во время обучения, из файла `vocabulary.ser`.
2.  **Чтение нового кода:** Программа читает новый, незнакомый фрагмент кода из файла (например, `input_code.txt`).
3.  **`CodeAnalyzer.analyze()`:**
    *   **Предобработка:** Используя **загруженный** `Vocabulary`, новый код токенизируется, конвертируется в индексы и паддится. Все слова, которых нет в словаре, заменяются на `[UNK]`.
    *   **`TransformerModel.forward()`**: Запускается **только прямой проход** модели. Данные проходят через `EmbeddingTable`, `EncoderBlock`-и и `ClassificationHead`, чтобы получить логиты (функцию потерь, вектор размером [1, numClasses], в зависимости от размера     переменная y в векторе может различаться). `backward` проход не вызывается.
    *   **`applySoftmax`**: Логиты преобразуются в вероятности.
    *   **Результат:** Находится индекс класса с максимальной вероятностью, который и является предсказанием модели.

---

## Часть 2: Инструкция по запуску

Для запуска проекта вам понадобится **Java JDK 17+** и **Apache Maven**.

### 1. Сборка проекта

Склонируйте репозиторий и соберите проект с помощью Maven. Это автоматически скачает все необходимые зависимости.
```bash
# Клонировать репозиторий
git clone https://github.com/HyperKa/Transformer.git
cd Transformer

# Собрать проект
mvn clean install

```
Далее, выбор датасета: игрушечный или основной. Если вы хотите выбрать основной датасет, необходимо закомментировать блок кода в классе ConfigConstants.java с 28 по 44 строки, и раскомментировать блок кода с 6 по 23 строки.
Если вы хотите выбрать игрушечный датасет для проверки корректности умножения матриц, оптимизации, мониторинга минимизации ошибок, то выполнить вышеупомянутые действия в противоположном порядке. Тогда будет активна только игрушечная конфигурация.

Далее есть два способа запуска приложения. Первый - запуск Main.java с уже существующими файлами vocabulary.ser и transformer_model.ser в корне проекта, нужно просто скомпилировать Main.java, обученная модель и словарь подтянутся из файлов с байт-кодами данных. Также необходимо заполнить класс input_code.txt, вставить в него код для анализа, чтобы получить предсказание его надежности. Второй - удаление vocabulary.ser и transformer_model.ser, тогда программа запустит процесс обучения из 20 эпох, файлы vocabulary.ser и transformer_model.ser создадутся заново и далее процесс идет как в первом способе.
